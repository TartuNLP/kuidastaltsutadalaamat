#!/usr/bin/env python3

import promptops

from aux import CmdlineArgs, log, load_model, load_tokenizer, env_stuff
from data import get_data_loader

import sys
import torch
import json
import torch.distributed as dist

from accelerate import Accelerator

from datetime import datetime

#from synthgen import filter_tr_pair

"""
This currently assumes the batch size to be 1. With larger batches the padding tokens went
into the decoder. Right-padding as a solution?
"""
def llm_generate(model, tokenizer, tok_batch, debug=False, max_len=2000, do_probs=False):
    if len(tok_batch['input_ids'][0]) > 1800:
        return ([''], -100) if do_probs else ['']

    tok_batch['input_ids'] = tok_batch['input_ids'].to(model.device)
    tok_batch['attention_mask'] = tok_batch['attention_mask'].to(model.device)
    start_time = datetime.now()

    if debug:
        log(f"Tokenized input: {tok_batch['input_ids']}")

    this_max_len = min(max_len, len(tok_batch['input_ids'][0])*5)

    raw_output_toks = model.generate(**tok_batch, tokenizer=tokenizer,
                                 do_sample=False, num_beams=4, max_length=this_max_len, top_p=None, temperature=None,
                                 eos_token_id=[tokenizer.eos_token_id,
                                               tokenizer.convert_tokens_to_ids("<|reserved_special_token_12|>"),
                                               tokenizer.convert_tokens_to_ids("<|reserved_special_token_13|>"),
                                               tokenizer.convert_tokens_to_ids("<|reserved_special_token_14|>"),
                                               tokenizer.convert_tokens_to_ids("<|reserved_special_token_15|>"),
                                               tokenizer.convert_tokens_to_ids("<|reserved_special_token_16|>")])

    #clean_output_toks = remove_prompt_from_output(tok_batch['attention_mask'], raw_output_toks, filler_id)
    assert len(raw_output_toks) == 1, "Only batch size=1 supported %-("
    gen_idx = len(tok_batch['attention_mask'][0])

    if debug:
        log(f"Full tokenized output: {raw_output_toks[0]}")
        log(f"Full tokens: {tokenizer.convert_ids_to_tokens(raw_output_toks[0])}")
        full_out = tokenizer.batch_decode([raw_output_toks[0]], skip_special_tokens=True)
        log(f"Full text: {full_out[0]}")

    clean_output_toks = raw_output_toks[0][gen_idx:]
    clean_outputs = tokenizer.batch_decode([clean_output_toks], skip_special_tokens=True)

    if debug:
        log(f"Pruned tokenized output: {clean_output_toks}")
        log(f"Pruned tokens: {tokenizer.convert_ids_to_tokens(clean_output_toks)}")
        log(f"Cleaned output: {clean_outputs[0]}")

        end_time = datetime.now()
        log(f"This took: {end_time - start_time}")

    if do_probs:
        meanlogprob = get_probs(model, raw_output_toks, tok_batch)
        return clean_outputs, meanlogprob
    else:
        return clean_outputs


def reassemble_multi(list_of_lists):
    result = []

    for gen_idx in range(len(list_of_lists[0])):
        for i in range(len(list_of_lists)):
            if gen_idx < len(list_of_lists[i]):
                result.append(list_of_lists[i][gen_idx])

    return result


#code mostly GPT-generated
def get_probs(model, outputs, inputs):
    logits = model(outputs).logits

    start = inputs["input_ids"].size(1)

    one_shift_logits = logits[:, :-1, :]  # [1, L-1, V]
    targets = outputs[:, 1:]  # [1, L-1]

    # only score the generated continuation
    cont_logits = one_shift_logits[:, start - 1:, :]  # [1, T, V]
    cont_targets = targets[:, start - 1:]  # [1, T]

    logprobs = torch.log_softmax(cont_logits, dim=-1)
    chosen = logprobs.gather(-1, cont_targets.unsqueeze(-1)).squeeze(-1)

    mean_logprob = chosen.mean().item()

    return mean_logprob


def predict(model, tokenizer, data_loader, accel,
            multi=False, debug=False, max_len=2000, sync=False, filter_eurollm=False, do_probs=False, cmdline_args=None):
    if cmdline_args is not None:
        multi, debug, sync, filter_eurollm, max_len, do_probs = (cmdline_args.multiproc, cmdline_args.debug,
                     cmdline_args.synchronize, cmdline_args.filter_eurollm, cmdline_args.max_len, cmdline_args.do_probs)

    outs_final = []

    with torch.no_grad():
        for idx, batch_tuple in enumerate(data_loader):
            batch, _, json_input_entry = batch_tuple

            if idx % accel.num_processes == accel.process_index or not multi:
                if multi and sync and (accel.num_processes > 1) and (idx // accel.num_processes) % 10 == 0:
                    # sync procs now, otherwise waiting times out in the end
                    wait_start_time = datetime.now()
                    accel.wait_for_everyone()
                    wait_end_time = datetime.now()
                    log(f"Waited for {wait_end_time - wait_start_time}")

                start_time = datetime.now()

                if do_probs:
                    outputs, avg_prob = llm_generate(model, tokenizer, batch, debug=debug, max_len=max_len, do_probs=True)
                    json_input_entry['hyp-mean-logprob'] = avg_prob
                else:
                    outputs = llm_generate(model, tokenizer, batch, debug=debug, max_len=max_len)
                end_time = datetime.now()

                log(f"Generated for {idx} in proc {accel.process_index} in {end_time - start_time}")
                new_entry = { **json_input_entry, 'hyp-output': outputs[0], 'hyp-index': idx }

                if filter_eurollm:
                    from synthgen import filter_tr_pair
                    new_entry['flt'] = filter_tr_pair(new_entry['hi_segm'],
                                                      new_entry['hyp-output'],
                                                      new_entry['hi_lang'],
                                                      new_entry['new_hi_res_lang'])
                outs_final.append(new_entry)

    if multi and sync:
        accel.wait_for_everyone()

        rank0_buffer = [None] * accel.num_processes if accel.is_main_process else None
        dist.gather_object(outs_final, rank0_buffer, dst=0)

        if accel.is_main_process:
            outs_final = reassemble_multi(rank0_buffer)
        else:
            outs_final = None

    return outs_final


def _cmdline_args():
    inputs = sys.argv[1:]

    description = """Predict output for an input via prompting"""

    pos_args = ["mdl_id"]

    #post-process the arguments
    args = CmdlineArgs(description, pos_args, input_args=inputs,
                       kw_arg_dict={"debug": False,
                                    "input_file": "none",
                                    "output_file": "none",
                                    "multiproc": False,
                                    "synchronize": True,
                                    "max_len": 2000,
                                    "filter_eurollm": False,
                                    "do_probs": False,
                                    "prompt_format": promptops.PF_ALPACA})

    if args.input_file == "none":
        args.input_file = None
    if args.output_file == "none":
        args.output_file = None

    log(f"Launched as {args}")

    return args


def save_all(outputs, args, acc):
    if acc.is_main_process or not args.synchronize:
        if not args.synchronize:
            out_fh = open(f"{args.output_file}.proc{acc.process_index}", "w")
        elif args.output_file is None:
            log("Writing to STDOUT")
            out_fh = sys.stdout
        else:
            out_fh = open(args.output_file, "w")

        if args.prompt_format in {promptops.PF_RAW, promptops.PF_RAWLINES}:
            for line in outputs:
                out_fh.write(str(line['hyp-output']) + "\n")
        else:
            json.dump(outputs, out_fh, indent=2, ensure_ascii=False)


def and_i_called_this_function_do_main_too():
    args = _cmdline_args()

    if args.multiproc:
        env_stuff()

    acc = Accelerator()
    device = acc.device

    log(f"Device: {device}.", accelerator=acc)

    if not args.multiproc and not acc.is_main_process:
        log("Not launched in multi-processing mode, exiting non-main process.")
        sys.exit(0)

    tokenizer = load_tokenizer(args.mdl_id, acc)

    data_loader = get_data_loader(args.input_file, args.prompt_format, tokenizer, debug=args.debug)

    model = load_model(args.mdl_id, device, acc, attention="eager")
    model.eval()

    log(f"Device: {model.device}.", accelerator=acc)

    log("Model loaded, starting to generate")
    with torch.no_grad():
        outputs = predict(model, tokenizer, data_loader, acc, cmdline_args=args)

    save_all(outputs, args, acc)

    log("Done")


if __name__ == "__main__":
    and_i_called_this_function_do_main_too()
